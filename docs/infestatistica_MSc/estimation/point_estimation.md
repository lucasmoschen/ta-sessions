# Estima√ß√£o pontual

Um **estimator pontual** de uma $g(\theta)$ √© uma estat√≠stica que toma valores em $\operatorname{Imagem}(g)$.
Mais formalmente:

> Sejam $\Omega$ espa√ßo de par√¢metros de uma fam√≠lia param√©trica de distribui√ß√µes $P_{\theta}$ e $g : \Omega \to G$ uma fun√ß√£o mensur√°vel (cont√≠nua, por exemplo). 
Uma fun√ß√£o $\phi : \mathcal{X} \to G' \supseteq G$ √© **estimador** de $g(\theta)$, em que $\mathcal{X}$ √© o espa√ßo amostral.

Vamos explorar formas de comparar estimadores pontuais.

## Vi√©s de um estimador

> Dizemos que $\phi$ √© **n√£o enviesado** se 
> $$
\mathbb{E}_{\theta}[\phi(X)] = g(\theta), \forall \theta \in \Omega.
> $$
> Al√©m do mais, o vi√©s de um estimador √© dado por $b_{\phi}(\theta) = \mathbb{E}_{\theta}[\phi(X)] - g(\theta)$.
> Se existe um estimador n√£o enviesado para $g(\theta)$, dizemos que $g$ √© U-estim√°vel.

Um estimador √© uma fun√ß√£o das amostras, enquanto uma **estimativa** √© uma avalia√ß√£o dessa fun√ß√£o em observa√ß√µes.

---
``üìù`` **Exemplo (Normal)**

Seja $X_1 \dots, X_n \overset{iid}{\sim} Normal(\mu, \sigma^2)$, em que $\theta = (\mu, \sigma^2) \in \mathbb{R} \times \mathbb{R}_+$ √© o par√¢metro.
Suponha que estamos interessados em $g(\theta) = \mu$.
Um estimador para $\mu$ √© $\phi(X_1, \dots, X_n) = X_1$ ou $\phi(X_1, \dots, X_n) = \min\{X_i\}$. 
Todavia, um estimador com melhores propriedades, que veremos ao decorrer do curso, √© $\phi(X_1, \dots, X_n) = \bar{X}$. 
Note que 
$$
\mathbb{E}_{\mu}[\bar{X}] = \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{\mu}[X_i] = \mu,
$$
o que mostra que $\bar{X}$ √© n√£o enviesado.

---

Aparentemente, um estimador ser n√£o enviesado parece muito bom, n√£o √©? Mas nem sempre isso √© suficiente ou poss√≠vel. 
No exemplo a seguir, mostramos que n√£o existe estimador n√£o enviesado para a taxa da distribui√ß√£o exponencial.

---
``üìù`` **Exemplo (Exponencial)**

Seja $X \sim Exponencial(\lambda)$, em que $\lambda \in \mathbb{R}_+$ √© o par√¢metro. 
Seja $\phi(X)$ um estimador n√£o enviesado para $\lambda$.
Ent√£o
$$
\mathbb{E}_{\lambda}[\phi(X)] = \lambda \int_0^{+\infty} \phi(x) \exp(-\lambda x) \, dx = \lambda, \forall \lambda \in \mathbb{R}_+. 
$$
Dividindo ambos os lados por $\lambda$ e diferenciando com respeito a $\lambda$, pela [Regra de Leibniz](https://en.wikipedia.org/wiki/Leibniz_integral_rule):
$$
- \int_0^{+\infty} x\phi(x) \exp(-\lambda x) \, dx = 0.
$$
Mas $x\exp(-\lambda x) > 0, \forall x > 0$. 
Se $\phi(x) > 0$ em um conjunto de medida positiva, pelo que estudamos em [Integra√ß√£o](https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/probability/#integracao), ter√≠amos que a integral deveria ser positiva, o que √© um absurdo. 
Com isso $\phi(x) = 0$ quase certamente e, portanto, n√£o √© um estimador n√£o enviesado. 
Conclu√≠mos que n√£o pode haver um estimador n√£o enviesado para $\lambda$.

---

Um estimador ser n√£o enviesado implique que, na pr√°tica, se fossem feitos infinitos experimentos e calcul√°ssemos o valor do estimador, o valor m√©dio convergiria para o valor verdadeiro do par√¢metro. 
Isso ocorre pela Lei dos Grandes N√∫meros.
Mas na pr√°tica, fazemos uma quantidade finita de experimentos e isso pode impactar nossos resultados. 
No pr√≥ximo exemplo, vamos mostras que um estimador n√£o enviesado e vamos visualizar sua distribui√ß√£o.

---
``üìù`` **Exemplo (Correla√ß√£o normal multivariada)**

Seja $(X_1, Y_1), \dots, (X_n,Y_n) \sim Normal(\boldsymbol{\mu}, \Sigma)$, com m√©dia $\mu = (0,0)$ e matriz de correla√ß√£o $\Sigma = [[1, \rho], [\rho, 1]]$. 
Assim o par√¢metro de interesse √© a correla√ß√£o entre $X$ e $Y$ $\rho \in [-1,1]$.
Considere o estimador para $\rho$ para $n$ amostras sendo
$$
\phi(X_1, \dots, X_n) = \dfrac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n (X_i - \bar{X})^2}\sqrt{\sum_{i=1}^n (Y_i - \bar{Y})^2}},
$$
conhecido como **correla√ß√£o emp√≠rica** ou **correla√ß√£o de Pearson**.
Apesar de famoso, esse estimador tem vi√©s n√£o nulo. 
Todavia, pode-se corrigi-lo para obtermos um estimador n√£o enviesado [(Olkin and Pratt, 1958)](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-29/issue-1/Unbiased-Estimation-of-Certain-Correlation-Coefficients/10.1214/aoms/1177706717.full),
da forma
$$
\psi = G(\phi) = \phi F\left(\frac{1}{2}, \frac{1}{2}; (n-1)/2; 1 - \phi^2\right),
$$
que tem a propriedade de n√£o ser enviesado, em que $F$ √© a fun√ß√£o hipergeom√©trica de Gauss.
Como $\phi$ e $\psi$ s√£o fun√ß√µes mensur√°veis das amostras, eles tamb√©m t√™m uma distribui√ß√£o amostral.
Uma maneira de visualizar essas distribui√ß√µes √© atrav√©s de simula√ß√µes de Monte Carlo.

1) Geramos $n$ amostras $(X_i, Y_i)$ da normal multivariada.
2) Calculamos $\phi$ e $\psi$. 
3) Repetimos esse processo $M$ vezes e temos amostras da distribui√ß√£o de $\phi$ e $\psi$.

```python
# Par√¢metros
rho = 0.1
n = 30
M = 100000
phi_samples = np.zeros(M)
psi_samples = np.zeros(M)
# Gerando os dados de acordo com a distribui√ß√£o especificada
rng = np.random.RandomState(1001)
for i in range(M):
    Z = rng.multivariate_normal(mean=[0,0], cov=[[1,rho], [rho,1]], size=n)
    X = Z[:,0]
    Y = Z[:,1]
    # Calculando ~ correla√ß√£o de Pearson
    phi_samples[i] = np.corrcoef(X,Y)[0,1]
    psi_samples[i] = phi_samples[i] * hyp2f1(1/2, 1/2, (n-1)/2, 1 - phi_samples[i]**2)

# Desenhando as distribui√ß√µes
plt.hist(phi_samples, label=r'$\phi$', color='pink', bins=50)
plt.hist(psi_samples, label=r'$\psi$', color='blue', bins=50)
plt.legend()
plt.show()
```

![png](output_3_0.png)
    

Nesse caso, temos que o Erro Absoluto Percentual M√©dio do estimador $\psi$ √©, aproximadamente,  
$$
\mathbb{E}_{\rho}\left[\bigg|\frac{\psi - \rho}{\rho}\bigg|\right] \approx 150\%.
$$
Ou seja, apesar de $\psi$ ser n√£o enviesado e $\phi$ ter vi√©s baixo, a vari√¢ncia dos estimadores √© bem grande. 
Nesse caso, temos uma probabilidade alta de obter uma estima√ß√£o 100% maior ou menor do que o valor verdadeiro de $\rho$.

Note que a diferen√ßa entre os estimadores √© pequena, pois $n$ √© razoavelmente grande e $\psi$ tem vi√©s baixo.

---

### Erro quadr√°tico

Seja $L(\theta, d) = (\theta - d)^2$ a perda quadr√°tica.
A [fun√ß√£o de risco](https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/risk_function/) $R(\theta, \phi)$ para o estimador $\phi(X)$ √© dada por 
$$
\begin{split}
R(\theta, \phi) &= \mathbb{E}_{\theta}[(\theta-\varphi(X))^2] \\
&= \mathbb{E}_{\theta}[(\theta-\mathbb{E}[\varphi(X)] + \mathbb{E}[\varphi(X)] - \varphi(X))^2] \\
&= (\theta-\mathbb{E}[\varphi(X)])^2 + 2(\theta-\mathbb{E}[\varphi(X)])(\mathbb{E}_{\theta}[\mathbb{E}[\varphi(X)] - \varphi(X))] + \mathbb{E}_{\theta}[(\mathbb{E}[\varphi(X)] - \varphi(X))^2] \\ 
&= b_{\phi}(\theta)^2 + 2(\theta-\mathbb{E}[\varphi(X)])(\mathbb{E}[\varphi(X)]-\mathbb{E}[\varphi(X)]) + \operatorname{Var}(\phi(X)) \\
&= b_{\phi}(\theta)^2 + \operatorname{Var}(\phi(X)).
\end{split}
$$
Portanto, para estimadores n√£o enviesados, o risco √© dado somente pela vari√¢ncia do estimador.

## Estima√ß√£o n√£o enviesada de menor vari√¢ncia

Note que quando queremos avaliar o melhor estimador, estamos lidando com uma classe bem grande para comparar. 
Por exemplo, o estimador $\hat{\theta} = 0$ para todo $\theta$ tem o menor MSE em $\theta = 0$, mas √© um estimador horr√≠vel quando $\theta$ se afasta de zero.
Nesse, sentido uma forma de avaliar estimadores √© restringindo a classe de interesse. 
Uma classe considerada razo√°vel √© a dos estimadores n√£o enviesados.
Nesse caso, os MSEs ser√£o iguais √†s vari√¢ncias dos estimadores.

> Um estimador n√£o enviesado $\phi$ √© **uniformly minimum variance unbiased estimator (UMVUE)** (estimador n√£o enviesado de vari√¢ncia uniformemente m√≠nima) se $\phi$ tem vari√¢ncia finita e, para todo estimador n√£o enviesado, $Var_{\theta} \phi(X) \le \operatorname{Var}_{\theta} \psi(X), \forall \theta \in \Omega$.

Essa defini√ß√£o leva ao seguinte importante teorema:

**Teorema (Lehmann-Scheff√©):** Se $T$ √© uma estat√≠stica completa, ent√£o todos os estimadores n√£o enviesados de $g(\theta)$, que s√£o fun√ß√µes de $T$, mas n√£o de $X$, s√£o iguais quase certamente para todo $\theta$.
Al√©m disso, se existe um estimador n√£o enviesado que √© fun√ß√£o de uma estat√≠stica suficiente completa, ent√£o ele √© UMVUE.

---
Ideia da prova: Sejam $\phi_1(T)$ e $\phi_2(T)$ estimadores n√£o enviesados de $g(\theta)$. 
Ent√£o $\mathbb{E}_{\theta}[\phi_1(T) - \phi_2(T)] = 0$ para todo $\theta$.
Como $T$ √© uma estat√≠stica completa, vale que $\phi_1(T) = \phi_2(T)$ quase certamente.
Agora suponha que $T$ √© estat√≠stica suficiente completa e seja $\phi(T)$ um estimador n√£o enviesado.
Tome um estimador n√£o enviesado $\psi(X)$ e defina $\phi'(T) = \mathbb{E}[\psi(X)|T]$. Usando a perda quadr√°tica, pelo [Teorema de Rao-Blackwell](https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/sufficiency/sufficiency/#teorema-de-rao-blackwell), $\operatorname{Var}_{\theta}(\phi ') = R(\theta, \phi ') \le R(\theta, \psi) = \operatorname{Var}_{\theta}(\psi)$ para todo $\theta$.
Portanto $\phi '$ √© UMVUE. Al√©m disso, como acabamos de provar, $\phi = \phi '$ quase certamente e, portanto, tamb√©m √© UMVUE.

---

Note que basta a exist√™ncia de um estimador n√£o enviesado $\delta$ e de uma estat√≠stica completa $T$ para que encontremos um estimador UMVUE usando Rao-Blackwell:
$$
\phi(T) = \mathbb{E}_{\theta}[\delta(X) | T].
$$

Considere o exemplo para o caso normal.

---
``üìù`` **Exemplo (Distribui√ß√£o normal - ela de novo)**

Seja $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$ e $\theta = (\mu, \sigma^2)$.
Ent√£o
$$
\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i, S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2
$$
s√£o estat√≠sticas suficientes e completas, al√©m de serem estimadores n√£o enviesados para $\mu$ e $\sigma^2$ respectivamente.
Pelo Teorema acima, qualquer fun√ß√£o deles √© um UMVUE pelo resultado acima, para $\mu$ e $\sigma^2$, respectivamente.
Todavia, √© poss√≠vel verificar que $S^2$ n√£o minimiza o erro quadrado.

*Obs.: para mostrar a sufici√™ncia, o Teorema da Fatoriza√ß√£o √© suficiente. J√° a completude vem do fato de que o espa√ßo de par√¢metros natural cont√©m um conjunto aberto em $\mathbb{R}^2$*.

---

Quando n√£o existem estat√≠sticas completas, o seguinte resultado pode ser √∫til:

**Teorema:** Uma condi√ß√£o suficiente e necess√°ria para que um estimador $\delta$ de $\mathbb{E}_{\theta} \delta(X)$ seja UMVUE √© que para todo $U$ que satisfa√ßa $\forall \theta, \mathbb{E}_{\theta}[U] = 0$, valha que $\operatorname{Cov}_{\theta}(\delta(X), U(X)) = 0$

---
``üìù`` **Exemplo (Bernoulli)**

Seja $X_1, \dots, X_n \sim \overset{iid}{\sim} Bernoulli(\theta)$.
Temos que $T(X) = \sum_{i=1}^n X_i$ √© uma estat√≠stica suficiente completa para $\theta$.
Al√©m disso, sabemos que $T(X) \sim Binomial(n,\theta)$.
Em particular, $T$ √© estat√≠stica completa, pois o espa√ßo de par√¢metros natural da distribui√ß√£o binomial tem interior n√£o vazio.
Considere $g(\theta) = \theta^2$. 
Um estimador n√£o enviesado para $g$ √© $\delta(X) = X_1 X_2$.
Nesse caso, o estimador UMVUE de $g$ √© $\mathbb{E}[\delta(X)|T]$, isto √©, 
$$
\phi(T) = \mathbb{E}_{\theta}[X_1X_2|T] = \mathbb{P}_{\theta}(X_1 = 1, X_2 = 1 | T = t).
$$
Podemos calcular essa probabilidade pela defini√ß√£o de probabilidade condicional, mas vamos fazer isso intuitivamente.
Temos $n$ espa√ßos e $t$ bolinhas para preencher $t$ desses espa√ßos. 

- Quantas formas eu tenho de posicionar as bolinhas? $n \choose t$. 

- Quantas dessas formas eu tenho interesse? Posiciono duas bolinhas nas primeiras posi√ß√µes (veja que preciso de $t \ge 2$) e tenho $n-2 \choose t-2$ formas de posicionar as outras bolinhas. Portanto:

$$
\mathbb{P}_{\theta}(X_1 = 1, X_2 = 1 | T = t) = \frac{(n-2)(n-3)\cdots(n-t+1)\cdot t(t-1)\cdots 1}{(t-2)(t-1)\cdots 1 \cdot n(n-1)\cdots(n-t+1)} = \frac{t(t-1)}{n(n-1)}.
$$

Portanto, o estimador $\phi(T) = T(T-1)/(n^2 - n)$ se $T \ge 2$ e $\phi(T) = 0$ se $T \le 1$ √© UMVUE para $g(\theta) = \theta^2$.

---

## M√©todo dos Momentos

O m√©todo de momentos √© uma abordagem para derivar estimadores para os par√¢metros do modelo a partir dos dados.
Ele √© simples para ser definido, mas pode apresentar v√°rios problemas como estimador.
Considere uma amostra aleat√≥ria $X = (X_1, \dots, X_n)$ com densidade $f_n(x|\theta)$, com $\theta \in \Omega \subseteq \mathbb{R}^k$.
Ent√£o, o estimador do m√©todo de momentos para $\theta$ √© a solu√ß√£o do sistema 
$$
m_i = \mu_i(\theta), i=1,\dots,k,
$$
em que $m_i = \sum_{j=1}^n X_j^i$ √© o $i$-√©simo momento emp√≠rico da amostra e $\mu_i(\theta) = \mathbb{E}_{\theta}[X^i]$, 
isto √©, dada uma observa√ß√£o, estamos igualando momentos emp√≠ricos aos momentos da distribui√ß√£o.

Um problema com o m√©todo de momentos √© que ele nem sempre prov√™ estimativas no espa√ßo dos par√¢metros. 
Isso pode acontecer se a variabilidade nos dados √© muito grande.

## Estimador de M√°xima Verossimilhan√ßa

Outro m√©todo para derivar estimadores √© o de maximar a fun√ß√£o de verossimilhan√ßa.

> Seja $X$ uma vari√°vel aleat√≥ria cuja distribui√ß√£o tem densidade $f(x|\theta)$.
Se $X=x$ √© observado, a fun√ß√£o do par√¢metro $L(\theta|x) = f(x|\theta)$ √© a **fun√ß√£o de verossimilhan√ßa**.
O **estimador de m√°xima verossimilhan√ßa (MLE)** de $\theta$ √© o valor 
$$
\hat{\theta} = \operatorname{arg}\max_{\theta \in \Omega} f(x|\theta),
$$
isto √©, √© o valor do par√¢metro em $\Omega$ que maximiza a fun√ß√£o de verossimilhan√ßa.

---
``üìù`` **Exemplo (Uniforme)**

Considere $X_1, \dots, X_n \overset{iid}{\sim} \operatorname{Unif}(0,\theta)$, cuja densidade √© 
$$
f_n(x|\theta) = \frac{1}{\theta^n}I_{[0,\theta]}(\max_{i} x_i).
$$
Supondo observada a amostra, queremos encontrar $\theta \ge \max_i x_i$ (caso contr√°rio, a densidade se anularia) que maximize $\theta^{-n}$. 
Isso acontece quando $\theta$ √© m√≠nimo. 
Nesse caso, $\hat{\theta} = \max_i x_i$.
Observe que se tiv√©ssemos tomado intervalo aberto, ao inv√©s de fechado, ter√≠amos que $\theta > \max_i x_i$, mas n√£o poderia ser igual de fato.
Nessa situa√ß√£o, n√£o existe MLE, visto que para qualquer $\theta > \max_i x_i$, existe $\theta '$ tal que $\theta > \theta ' > \max_i x_i$.

---

Mesmo quando o MLE existe, n√£o h√° garantias de que ele √© √∫nico.
Por√©m, se ele existe, ele satisfaz a seguinte propriedade chamada de **invari√¢ncia**

**Teorema:** Seja $g : \Omega \to G$ uma fun√ß√£o mensur√°vel.
Suponha que exista um espa√ßo $U$ e uma fun√ß√£o $h : \Omega \to G \times G'$ bijetiva tal que $h(\theta) = (g(\theta), g'(\theta))$ para alguma fun√ß√£o $G$.
Se $\hat{\theta}$ √© MLE de $\theta$, ent√£o $g(\hat{\theta})$ √© MLE de $g(\theta)$.

Note que uma consequ√™ncia direta √© que transforma√ß√µes bijetivas levam MLEs em MLEs.

Note que $\hat{\theta}$ maximiza a verossimilhan√ßa se, e somente se, maximiza o logaritmo da verossimilhan√ßa. 
Com isso, temos uma abordagem mais simples para encontrar o MLE de distribui√ß√µes da fam√≠lia exponencial, pois 
$$
\log L(\theta|x) = \log h(x) - A(\theta) + x\cdot \theta.
$$
Se o MLE $\hat{\theta}$ ocorre no interior do espa√ßo de par√¢metros natural, as derivadas parciais de $\log L(\theta)$ se anulam em $\theta = \hat{\theta}$.
Em particular, 
$$
x = \nabla A(\theta) = \mathbb{E}_{\theta}[X],
$$
como visto [aqui](https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/exponential_family/exponential_family/#identidade-para-os-momentos), o que implica que o MLE de $\theta$ √© o valor tal que $x = \mathbb{E}_{\hat{\theta}}[X]$.

Portanto o procedimento padr√£o para encontrar o MLE √© o seguinte:

1. Verificar se a verossimilhan√ßa tem alguma estrutura que facilite maximizar, como no caso da distribui√ß√£o uniforme.

2. Tomar o logaritmo da verossimilhan√ßa e encontrar o ponto cr√≠tico derivando e igualando a zero para pontos no interior do espa√ßo de par√¢metros.

3. Verificar condi√ß√µes de sufici√™ncia para os pontos cr√≠ticos. Por exemplo: segunda derivada negativa (ou Hessiana no caso de multivariada negativa definida). 

4. Verificar a fronteira se necess√°rio.

5. Utilizar recursos num√©ricos.